Optimization:

Gradient Descent, SGD, mini-batch SGD
Momentum increases the strength of updates for dimensions who gradients point in the same direction
, which is commonly set to 0.9 or set to 0.5 then increase it to 0.9
Nesterov’s Acceleration：as a corrective update to the momentum.

Recommendations: 
1. Whenever using SGD, also apply momentum.
2. For Nesterov acceleration, on smaller datasets.


Regularization:
L1, L2, Elastic Net, dropout, data augmentation, early stopping
